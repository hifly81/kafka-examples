= Apache Kafka examples
==============================
:toc:
:toc-placement: preamble
:toclevels: 2
:showtitle:
:Some attr: Some value

// Need some preamble to get TOC:
{empty}


== Apache Kafka installation

Details about the installation, info at: +
https://kafka.apache.org/documentation/#quickstart

== Apache Kafka using Docker

To run a kafka cluster with Docker, use the _docker-compose.yml_ file available in the root directory.

Images are downloaded from _confluentinc_ and are based on _Confluent 7.0.1_ version (kafka version 3.0):

* Zookeeper: confluentinc/cp-zookeeper:7.0.1
* Kafka: confluentinc/cp-kafka:7.0.1
* Schema Registry: confluentinc/cp-schema-registry:7.0.1
* Connect: custom image based on confluentinc/cp-kafka-connect-base:7.0.1
* ksqlDB server: confluentinc/ksqldb-server:0.21.0
* ksqlDB cli: confluentinc/ksqldb-cli:0.21.0
* PostgreSQL: postgres:10.5

Components list:

* Broker will listen to _localhost:29092_
* Schema Registry will listen to _localhost:8081_
* Connect will listen to _localhost:8083_
* ksqlDB cli will listen to _localhost:8088_
* PostgreSQL will listen to _localhost:5432_

=== Start containers +

[source,bash]
----
./bootstrap.sh

----

=== Stop containers +

[source,bash]
----
./tear-down.sh

----

== Apache Kafka on Kubernetes using Confluent For Kubernetes

To run a kafka cluster on Kubernetes, have a look at Confluent for Kubernetes operator (CFK).

Example of bootstrapping a cluster using CFK:

link:confluent-for-kubernetes/README.adoc[CFK Example]

== Kafka producers

Some implementations of kafka producers.


=== String +

It uses _org.apache.kafka.common.serialization.StringSerializer_ for key and value

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.string.Runner"
----

=== JSON +

It uses _org.apache.kafka.common.serialization.StringSerializer_ for key and a _org.hifly.kafka.demo.producer.serializer.json.JsonSerializer_ for value

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.json.Runner"
----

=== Avro-confluent +

It uses _io.confluent.kafka.serializers.KafkaAvroSerializer_ for value and a GenericRecord.

Confluent schema registry is needed tu run the example. +

More Info at: https://github.com/confluentinc/schema-registry

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.avro.RunnerConfluent"
----

=== Avro-apicurio +

It uses _io.apicurio.registry.utils.serde.AvroKafkaSerializer_ for value and a GenericRecord.

Apicurio schema registry is needed tu run the example. +

Info at: https://github.com/Apicurio/apicurio-registry

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.avro.RunnerApicurio"
----

=== Partitioner +

It uses a custom partitioner for keys.

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.partitioner.custom.Runner"
----

Execute tests:

[source,bash]
----
cd kafka-producer
mvn clean test
----


== Kafka consumers

Implementation of a kafka consumer that can be used with different deserializer classes (for key and value).

Execute tests:

[source,bash]
----
cd kafka-consumer
mvn clean test
----

Every consumer implementation has its own _Runner_ java class consuming a bunch of sample messages.

[source,bash]
----
cd kafka-consumer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.deserializer.Runner"
----

== Confluent Avro Specific Record

Implementation of a kafka producer and a kafka consumer using Avro Specific Record for serializing and deserializing.

Confluent schema registry is needed tu run the example. +

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars--replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the producer:

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerProducer"
----

Run the consumer:

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerConsumer"
----

== Kafka streams

Implementation of a series of kafka streams topologies.

Execute tests:

[source,bash]
----
cd kafka-streams
mvn clean test
----

=== StreamCounter +
Count number of events grouped by key.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic counter-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic counter-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.StreamCounter"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic counter-input-topic --property "parse.key=true" --property "key.separator=:"
"John":"transaction_1"
"Mark":"transaction_1"
"John":"transaction_2"
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic counter-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : " --value-deserializer "org.apache.kafka.common.serialization.LongDeserializer"
----

=== StreamSum +
Sum values grouping by key.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.StreamSum"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic sum-input-topic --property "parse.key=true" --property "key.separator=:"
"John":1
"Mark":2
"John":5
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic sum-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : " --value-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"
----

=== CarSensorStream +
The stream filters out speed data from car data sensor records. Speed limit is set to 150km/h and only events exceeding the limits are filtered out. +
A ktable stores the car info data. +
A left join between the kstream and the ktable produces a new aggregated object published to an output topic.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic carinfo-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic carsensor-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic carsensor-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.CarSensorStream"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic carinfo-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
----

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic carsensor-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","speed":350}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic carsensor-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

=== CarBrandStream +
The stream splits the original data into 2 different topics, one for Ferrari cars and one for all other car brands.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic ferrari-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic cars-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.CarBrandStream"
----

Send messages to input topic:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic cars-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
2:{"id":"2","brand":"Bugatti","model":"Chiron"}
----

Read from output topics:

[source,bash]
----
kafka-console-consumer --topic ferrari-input-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

[source,bash]
----
kafka-console-consumer --topic cars-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

== Kafka streams Processor API

Examples with Processor API.

=== JSONArrayRemoveProcessor +

Remove a specific json field from the record and forward it to the next topology node.

Execute tests:

[source,bash]
----
cd kafka-streams-processor
mvn clean test
----

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic processor-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic processor-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.processor.JSONArrayRemoveProcessorApplication"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic processor-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic processor-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

=== ExpiredMessagesProcessor +

Remove old entries based on time (expiration time 30 seconds) using a punctuator.

Execute tests:

[source,bash]
----
cd kafka-streams-processor
mvn clean test
----

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic expired-messages-input-topic--replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic expired-messages-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.processor.ExpiredMessagesApplication"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic expired-messages-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","remote-device":"R01","time":"2021-11-02T02:50:12.208Z"}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic expired-messages-input-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

== Kafka Orders App: Example with end-to-end exactly-once semantic between consumer and producer

Example of a cart application implementing end-to-end exactly-once semantic between consumer and producer. +
The ItemsProducer class sends 2 items in a single transaction. +
The ItemsConsumer class receives the items and creates an order containing the items. +
The consumer offset is committed only if the order can be created and sent.

Execute tests:

[source,bash]
----
cd kafka-orders-tx
mvn clean test
----

Execute the ItemsProducer class: 

[source,bash]
----
cd kafka-orders-tx
mvn clean compile && mvn exec:java -Dexec.mainClass="ItemsProducer"
----

Execute the ItemsConsumer class: 

[source,bash]
----
cd kafka-orders-tx
mvn clean compile && mvn exec:java -Dexec.mainClass="ItemsConsumer"
----

== Kafka Spring Boot

Sample of a kafka producer and consumer implemented with Spring Boot 2.x.

Kafka Consumer implements a DLQ for records not processable (after 3 attempts).

Run on your local machine: 

[source,bash]
----
#start a producer on port 8010
cd kafka-springboot-producer
mvn spring-boot:run

#start a consumer on port 8090
cd kafka-springboot-consumer
mvn spring-boot:run

#Send orders (on topic demoTopic)
curl --data '{"id":5, "name": "PS5"}' -H "Content-Type:application/json" http://localhost:8010/api/order

#Send ERROR orders and test DLQ (on topic demoTopic)
curl --data '{"id":5, "name": "ERROR-PS5"}' -H "Content-Type:application/json" http://localhost:8010/api/order
----

== Kafka Quarkus

Sample of a kafka producer and consumer implemented with Quarkus.
Every 1s a new message is sent to demo topic.

Run on your local machine: 

[source,bash]
----
cd kafka-quarkus
./mvnw clean compile quarkus:dev (debug port 5005)
----

Run on Openshift machine: 

[source,bash]
----
cd kafka-quarkus
./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.kubernetes.deploy=true
----

== Kafka microprofile2

Sample of a kafka producer and consumer running on an open liberty MicroProfile v2 runtime.

Run on docker: 

[source,bash]
----
#Start a zookeeper container
docker run -d --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 debezium/zookeeper

#Start a kafka container
docker run -d --name my-cluster-kafka-bootstrap -p 9092:9092 --link zookeeper:zookeeper debezium/kafka

#Start a kafka producer container
cd kafka-microprofile2-producer
docker build -t kafka-producer:latest .
docker run -d --name kafka-producer -p 9080:9080 -e KAFKABROKERLIST=my-cluster-kafka-bootstrap:9092 --link my-cluster-kafka-bootstrap:my-cluster-kafka-bootstrap kafka-producer:latest

#Start a kafka consumer container
cd kafka-microprofile2-consumer
docker build -t kafka-consumer:latest .
docker run -d --name kafka-consumer -p 9090:9080 -e KAFKABROKERLIST=my-cluster-kafka-bootstrap:9092 --link my-cluster-kafka-bootstrap:my-cluster-kafka-bootstrap kafka-consumer:latest

#Receive orders
curl -v -X POST http://localhost:9090/kafka-microprofile2-consumer-0.0.1-SNAPSHOT/order

#Send orders (500)
curl -v -X POST http://localhost:9080/kafka-microprofile2-producer-0.0.1-SNAPSHOT/order
----

== Kafka unixstats connector

Implementation of a sample Source Connector; it executes _unix commands_ (e.g. _ls -ltr, netstat_) and sends its output to a kafka topic.

This connector relies on Confluent Schema Registry to convert the values using Avro: _CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter_.

Connector config is in _kafka-nixstats-connector/config/source.quickstart.json_ file.

Parameters for source connector:

- _command_ – nix command to execute (e.g. ls -ltr)
- _topic_ – output topic
- _poll.ms_ – poll interval in milliseconds between every execution

Create the connector package:

[source,bash]
----
cd kafka-nixtstats-connector
mvn clean package
----

Create a connect custom Docker image with the connector installed:

This will create an image based on _confluentinc/cp-kafka-connect-base:XXX_ using a custom _Dockerfile_.
It will use the Confluent utility _confluent-hub install_ to install the plugin in connect.

[source,bash]
----
/kafka-nixstats-connector/./build-image.sh
----

Run the Docker container:

[source,bash]
----
docker-compose up -d
----

Deploy the connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @kafka-nixstats-connector/config/source.quickstart.json
----

== ksqlDB Sample App

Implementation of a sample App (kafka producer and consumer) sending and receiving orders; ksqlDB acts as an orchestrator to coordinate a sample Saga.

Compile:

[source,bash]
----
cd ksqldb-sample
mvn schema-registry:download
mvn generate-sources
mvn clean compile
----

Launch on local environment:

Launch Docker Compose:

[source,bash]
----
docker-compose up
----

Connect to ksqlDB and set auto.offset.reset:

[source,bash]
----
ksql http://ksqldb-server:8088
SET 'auto.offset.reset' = 'earliest';
----

Create DDL on ksqlDB:

[source,bash]
----
/ksqldb-sample/ksql/./ksql-statements.sh
----

Create fat jar of Sample application (1 Saga):

[source,bash]
----
cd ksqldb-sample
mvn clean compile assembly:single
----

Execute fat jar of Sample application (1 Saga):

[source,bash]
----
cd ksqldb-sample
java -jar target/ksqldb-sample-0.0.1-SNAPSHOT-jar-with-dependencies.jar
----

Saga Verification:

Insert entries on ksqlDB:

[source,bash]
----
ksql http://ksqldb-server:8088
----

[source,sql]
----
insert into accounts values('AAA', 'Jimmy Best');
insert into orders values('AAA', 150, 'Item0', 'A123', 'Jimmy Best', 'Transfer funds', '2020-04-22 03:19:51');
insert into orders values('AAA', -110, 'Item1', 'A123', 'amazon.it', 'Purchase', '2020-04-22 03:19:55');
insert into orders values('AAA', -100, 'Item2', 'A123', 'ebike.com', 'Purchase', '2020-04-22 03:19:58');

select * from orders_tx where account_id='AAA' and order_id='A123';
----

[source,java]
----
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": 0, "ACCOUNT": "AAA", "ITEMS": ["Item0"], "ORDER": "A123"}
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": 0, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1"], "ORDER": "A123"}
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": -1, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1", "Item2"], "ORDER": "A123"}
 --> compensate:{"TX_ID": "TX_AAA_A123", "TX_ACTION": -1, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1", "Item2", "ORDER": "A123"}
----

== CDC with Debezium PostgreSQL Source Connector

Usage of Debezium Source connector for PostgreSQL to send RDMS table updates into a kafka topic.

The _debezium/debezium-connector-postgresql:1.7.1_ connector has been installed into connect docker image using confluent hub (see _docker-compose.yml_ file).
More details on the connector are available at: https://docs.confluent.io/debezium-connect-postgres-source/current/overview.html.

The connector uses _pgoutput_ plugin for replication. This plug-in is always present in PostgreSQL server. The Debezium connector interprets the raw replication event stream directly into change events.

Verify the existence of _account_ table and data in PostgreSQL:

[source,bash]
----
docker exec -it postgres psql -h localhost -p 5432 -U postgres -c 'select * from accounts;'
----

Deploy the connector:

[source,bash]
----
curl -v -X POST -H 'Content-Type: application/json' -d @cdc-debezium-postgres/config/debezium-source-pgsql.json http://localhost:8083/connectors
----

Run a kafka consumer on _postgres.public.accounts_ topic and see the records:

[source,bash]
----
kafka-console-consumer --topic postgres.public.accounts --bootstrap-server localhost:9092 --from-beginning
----

Insert a new record into _account_ table:

[source,bash]
----
docker exec -it postgres psql -h localhost -p 5432 -U postgres -c "insert into accounts (user_id, username, password, email, created_on, last_login) values (3, 'foo3', 'bar3', 'foo3@bar.com', current_timestamp, current_timestamp);"
----

== Kafka OAUTH KIP-768

This example shows how to configure kafka to use SASL/OAUTHBEARER authentication with Support for OIDC.

To run the sample you need to run Keycloak server and configure openid-connect on it.

Run Keycloak server with PostgreSQL (on port 8080):

[source,bash]
----
docker-compose -f kafka-oauth-kip-768/docker-compose-oauth.yml up -d
----

Keycloak setup:

[source,bash]
----

 - Login to http://localhost:8080 (admin/Pa55w0rd)
 - Create a realm called kafka
 - From the Clients tab, create a client with Cliend ID "kafka_user".
 - Change Access Type to Confidential
 - Turn Standard Flow Enabled to OFF
 - Turn Service Accounts Enabled to ON
 - In the Advanced Settings below on the settings tab, set Access Token Lifespan to 10 minutes
 - Switch to the Credentials tab
 - Set Client Authenticator to "Client Id and Secret"
 - Copy the client-secret
 - Save
----

Apache Kafka 3.1 setup _(at the moment an official docker image is not available for kafka 3.1)_:

Add these properties to your kafka _server.properties_ and start kafka:

[source,bash]
----
listeners=PLAINTEXT://:9092,CLIENT://:9093
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL,CLIENT:SASL_PLAINTEXT

listener.name.client.sasl.enabled.mechanisms=OAUTHBEARER
listener.name.client.sasl.oauthbearer.jwks.endpoint.url=http://localhost:8080/auth/realms/kafka/protocol/openid-connect/certs
listener.name.client.sasl.oauthbearer.expected.audience=account
listener.name.client.oauthbearer.sasl.server.callback.handler.class=org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerValidatorCallbackHandler
listener.name.client.oauthbearer.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;
----

Run a kafka producer test using the _client-oauth-properties_ (add your client_secret into the file) on listener port 9093:

[source,bash]
----
kafka_2.12-3.1.0/bin/kafka-producer-perf-test.sh --topic my_topic --num-records 50 --throughput 10 --record-size 1 --producer-props bootstrap.servers=localhost:9093  --producer.config kafka-oauth-kip-768/client-oauth.properties
----

Teardown:

[source,bash]
----
docker-compose -f kafka-oauth-kip-768/docker-compose-oauth.yml down
----
