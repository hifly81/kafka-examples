= Apache Kafka examples
==============================
:toc:
:toc-placement: preamble
:toclevels: 3
:showtitle:
:Some attr: Some value

// Need some preamble to get TOC:
{empty}


== Installation with Docker

=== KRaft mode

To run a kafka cluster with Docker, use the _docker-compose-kraft.yml_ file available in the root directory.

Images are downloaded from _confluentinc_ and are based on _Confluent 7.4.0 Community_ licensed (kafka version 3.4.0):

* Kafka: confluentinc/cp-kafka
* Schema Registry: confluentinc/cp-schema-registry
* Connect: custom image based on confluentinc/cp-kafka-connect-base
* ksqlDB server: confluentinc/ksqldb-server:0.28.2
* ksqlDB cli: confluentinc/ksqldb-cli:0.28.2

Components list:

* Broker will listen to _localhost:9092_
* Schema Registry will listen to _localhost:8081_
* Connect will listen to _localhost:8083_
* ksqlDB cli will listen to _localhost:8088_


==== Start containers +

[source,bash]
----
scripts/bootstrap-kraft.sh

----

Check metadata quorum:

[source,bash]
----
docker exec -it broker kafka-metadata-quorum --bootstrap-server broker:9092 describe --status

ClusterId:              QTnB2tAgTWa1ec5wYon2jg
LeaderId:               1
LeaderEpoch:            1
HighWatermark:          387
MaxFollowerLag:         0
MaxFollowerLagTimeMs:   253
CurrentVoters:          [1]
CurrentObservers:       []
----

==== Stop containers +

[source,bash]
----
scripts/tear-down-kraft.sh
----

=== Zookeeper mode

To run a kafka cluster with Docker, use the _docker-compose.yml_ file available in the root directory.

Images are downloaded from _confluentinc_ and are based on _Confluent 7.4.0 Community_ licensed (kafka version 3.4.0):

* Zookeeper: confluentinc/cp-zookeeper
* Kafka: confluentinc/cp-kafka
* Schema Registry: confluentinc/cp-schema-registry
* Connect: custom image based on confluentinc/cp-kafka-connect-base
* ksqlDB server: confluentinc/ksqldb-server:0.28.2
* ksqlDB cli: confluentinc/ksqldb-cli:0.28.2

Components list:

* Broker will listen to _localhost:9092_
* Schema Registry will listen to _localhost:8081_
* Connect will listen to _localhost:8083_
* ksqlDB cli will listen to _localhost:8088_


==== Start containers +

[source,bash]
----
scripts/bootstrap.sh

----

==== Stop containers +

[source,bash]
----
scripts/tear-down.sh

----

==== Change Images Version

Change the docker image version for the specific component, updating file _.env_ in root directory.

== Installation on Kubernetes using Confluent For Kubernetes

Examples will run using:

 . Minikube
 . Confluent for Kubernetes operator (CFK): https://docs.confluent.io/operator/current/overview.html

Images are downloaded from _confluentinc_ and are based on _Confluent 7.4.0 Community_ licensed (kafka version 3.4.0):

* Kafka: confluentinc/cp-kafka
* Schema Registry: confluentinc/cp-schema-registry
* Connect: custom image based on confluentinc/cp-kafka-connect-base
* ksqlDB server: confluentinc/ksqldb-server:0.28.2
* ksqlDB cli: confluentinc/ksqldb-cli:0.28.2

=== Minikube installation in Linux with KVM/QEMU

Follow instructions for ArchLinux (also tested with Fedora)

https://dev.to/xs/kubernetes-minikube-with-qemu-kvm-on-arch-312a

=== Minikube installation in Mac/Windows

https://minikube.sigs.k8s.io/docs/start/

=== Deployment of a Kafka cluster with Confluent for Kubernetes Operator

Start Minikube with kvm2 driver:

[source,bash]
----
minikube delete
minikube config set driver kvm2
touch /tmp/config && export KUBECONFIG=/tmp/config
minikube start --memory 16384 --cpus 4
----

Create confluent k8s namespace:

[source,bash]
----
kubectl create namespace confluent
kubectl config set-context --current --namespace confluent
----

Add confluent repo to helm:

[source,bash]
----
helm repo add confluentinc https://packages.confluent.io/helm
helm repo update
----

==== Option 1: Install confluent-for-kubernetes operator from Confluent’s Helm repo:

[source,bash]
----
helm upgrade --install confluent-operator confluentinc/confluent-for-kubernetes
----

==== Option 2: Install confluent-for-kubernetes operator using the download bundle:

[source,bash]
----
wget https://confluent-for-kubernetes.s3-us-west-1.amazonaws.com/confluent-for-kubernetes-2.6.0.tar.gz
tar xvf confluent-for-kubernetes-2.6.0.tar.gz
helm upgrade --install confluent-operator ./confluent-for-kubernetes-2.6.0/helm/confluent-for-kubernetes
----


==== Install Confluent components: 1 zk, 3 brokers:

[source,bash]
----
kubectl apply -f confluent-for-kubernetes/k8s/confluent-platform-reducted.yaml

(master)$ kubectl get pods
NAME                                  READY   STATUS    RESTARTS   AGE
confluent-operator-665db446b7-j52rj   1/1     Running   0          6m35s
kafka-0                               1/1     Running   0          65s
kafka-1                               1/1     Running   0          65s
kafka-2                               1/1     Running   0          65s
zookeeper-0                           1/1     Running   0          5m5s

----

Verify events and pods:

[source,bash]
----
watch -n 5 "kubectl get events --sort-by='.lastTimestamp'"
watch -n 5 "kubectl get pods"
----

alternately, you can install additional Confluent components: 1 zk, 3 brokers, 1 connect, 1 ksqldb, 1 schema registry:

[source,bash]
----
kubectl apply -f confluent-for-kubernetes/k8s/confluent-platform.yaml
----

==== Kafka Operations

Topic create:

[source,bash]
----
kubectl exec --stdin --tty kafka-0 -- /bin/bash
kafka-topics --bootstrap-server localhost:9092 --create --topic test-1
----

Topic list:

[source,bash]
----
kubectl exec --stdin --tty kafka-0 -- /bin/bash
kafka-topics --bootstrap-server localhost:9092 --list
----

Topic describe:

[source,bash]
----
kubectl exec --stdin --tty kafka-0 -- /bin/bash
kafka-topics --bootstrap-server localhost:9092 --topic test-1 --describe
----

Produce messages to Topic:

[source,bash]
----
kubectl exec --stdin --tty kafka-0 -- /bin/bash
kafka-producer-perf-test --num-records 1000000 --record-size 1000 --throughput -1 --topic test-1 --producer-props bootstrap.servers=localhost:9092
----

Consume messages from Topic:

[source,bash]
----
kubectl exec --stdin --tty kafka-0 -- /bin/bash
kafka-console-consumer --bootstrap-server localhost:9092 --topic test-1 --from-beginning
----

==== Tear Down

Shut down Confluent Platform and the data:

[source,bash]
----
kubectl delete -f k8s/topic.yml
kubectl delete -f k8s/producer.yml
kubectl delete -f k8s/confluent-platform.yaml
helm delete confluent-operator
----

Delete namespace confluent:

[source,bash]
----
kubectl delete namespace confluent
----

== Kafka producers

Some implementations of kafka producers.

To launch the examples run kafka on port 9092:

=== StringSerializer +

It uses _org.apache.kafka.common.serialization.StringSerializer_ for key and value

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.string.Runner"
----

=== JsonSerializer +

It uses _org.apache.kafka.common.serialization.StringSerializer_ for key and a _org.hifly.kafka.demo.producer.serializer.json.JsonSerializer_ for value

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.json.Runner"
----

=== Transactional Producer [[tx_producer]] +

It uses _org.apache.kafka.common.serialization.StringSerializer_ for key and value and set _enable.idempoteceny_ and _transactional.id_

Create a topic with 3 partitions:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic test-idempotent --replication-factor 1 --partitions 3
----

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.tx.Runner"
----

=== Avro-confluent +

It uses _io.confluent.kafka.serializers.KafkaAvroSerializer_ for value and a GenericRecord.

Confluent schema registry is needed to run the example. +

More Info at: https://github.com/confluentinc/schema-registry

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.avro.Runner" -Dexec.args="CONFLUENT"
----

=== Avro-apicurio +

It uses _io.apicurio.registry.utils.serde.AvroKafkaSerializer_ for value and a GenericRecord.

Apicurio schema registry is needed to run the example. +

Info at: https://github.com/Apicurio/apicurio-registry

Start Apicurio:

[source,bash]
----
./scripts/bootstrap-apicurio.sh
----

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.avro.Runner" -Dexec.args="APICURIO"
----

Teardown:

[source,bash]
----
./scripts/teardown-apicurio.sh
----


=== Partitioner +

It uses a custom partitioner for keys.

[source,bash]
----
cd kafka-producer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.producer.serializer.partitioner.custom.Runner"
----

Execute tests:

[source,bash]
----
cd kafka-producer
mvn clean test
----

=== Interceptor

This example shows how to create a custom producer interceptor. class _CreditCardProducerInterceptor_ will mask a sensitive info on producer record (credit card).

Compile and package:

[source,bash]
----
cd interceptors
mvn clean package
----

Run a kafka producer on listener port 9092:

[source,bash]
----
mvn exec:java -Dexec.mainClass="org.hifly.kafka.interceptor.producer.Runner"
----

Run a kafka consumer on listener port 9092:

[source,bash]
----
mvn exec:java -Dexec.mainClass="org.hifly.kafka.interceptor.consumer.Runner"
----

Verify output:

[source,bash]
----
record is:XXXXXX
Topic: test_custom_data - Partition: 0 - Offset: 1
----

=== Python Producer

Install python lib _confluent-kafka_:

[source,bash]
----
pip install confluent-kafka
----

Create topic:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic kafka-topic --replication-factor 1 --partitions 1
----

Run producer:

[source,bash]
----
cd kafka-python-producer
python producer.py
----


== Kafka consumers

Implementation of a kafka consumer that can be used with different deserializer classes (for key and value).

Class _org.hifly.kafka.demo.consumer.deserializer.impl.ConsumerInstance_ can be customized with:

 - clientId _(string)_
 - groupId _(string)_
 - topics _(string separated by comma)_
 - key deserializer class _(string)_
 - value deserializer class _(string)_
 - partition assignment strategy _(org.apache.kafka.clients.consumer.RangeAssignor|org.apache.kafka.clients.consumer.RoundRobinAssignor|org.apache.kafka.clients.consumer.StickyAssignor|org.apache.kafka.clients.consumer.CooperativeStickyAssignor)_
 - isolation.level _(read_uncommitted|read_committed)_
 - poll timeout _(ms)_
 - consume duration _(ms)_
 - autoCommit _(true|false)_
 - commit sync _(true|false)_
 - subscribe mode _(true|false)_

Topics can be passed as argument 1 of the main program:
[source,bash]
----
-Dexec.args="users,users_clicks"
----

Partition assignment strategy can be passed as argument 2 of the main program:
[source,bash]
----
-Dexec.args="users,users_clicks org.apache.kafka.clients.consumer.RoundRobinAssignor"
----

Execute tests:

[source,bash]
----
cd kafka-consumer
mvn clean test
----

=== StringDeserializer

It uses _org.apache.kafka.common.serialization.StringDeserializer_ for key and value. Default topic is topic1.

[source,bash]
----
cd kafka-consumer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.core.Runner"
----

Send messages to the topic:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic topic1 --property "parse.key=true" --property "key.separator=:"
> Frank:1
----

=== read committed [[readcommitted_consumer]] +

It uses _org.apache.kafka.common.serialization.StringDeserializer_ for key and value and set _isolation.level_ to _read_committed_.

It must be only used with a transactional producer.

[source,bash]
----
cd kafka-consumer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.tx.Runner"
----

=== Consumer Partition Assignor

==== Range (default)

Create 2 topics with same number of partitions:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic users --replication-factor 1 --partitions 3
kafka-topics --bootstrap-server localhost:9092 --create --topic users_clicks --replication-factor 1 --partitions 3
----

Run 2 consumer instances (2 different shell/terminal) belonging to the same group and subscribed to _user_ and _user_clicks_ topics; consumers uses
_org.apache.kafka.clients.consumer.RangeAssignor_ to distribute partition ownership.

[source,bash]
----
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.core.Runner" -Dexec.args="users,users_clicks"
----

Send messages to the topics using the same key (Frank):

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic users --property "parse.key=true" --property "key.separator=:"
> Frank:1
kafka-console-producer --broker-list localhost:9092 --topic users_clicks --property "parse.key=true" --property "key.separator=:"
> Frank:1
----


Verify that the same consumer instance will read both messages.

[source,bash]
----
Group id group-XX - Consumer id: consumer-group-XX-1-421db3e2-6501-45b1-acfd-275ce8d18368 - Topic: users - Partition: 1 - Offset: 0 - Key: frank - Value: 1
Group id group-XX - Consumer id: consumer-group-XX-1-421db3e2-6501-45b1-acfd-275ce8d18368 - Topic: users_clicks - Partition: 1 - Offset: 0 - Key: frank - Value: 1
----

==== Round Robin

Create 2 topics with same number of partitions:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic users --replication-factor 1 --partitions 3
kafka-topics --bootstrap-server localhost:9092 --create --topic users_clicks --replication-factor 1 --partitions 3
----

Run 2 consumer instances (2 different shell/terminal) belonging to the same group and subscribed to _user_ and _user_clicks_ topics; consumers uses
_org.apache.kafka.clients.consumer.RoundRobinAssignor_ to distribute partition ownership.

[source,bash]
----
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.core.Runner" -Dexec.args="users,users_clicks org.apache.kafka.clients.consumer.RoundRobinAssignor"
----

Send messages to the topics using the same key (Frank):

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic users --property "parse.key=true" --property "key.separator=:"
> Frank:1
kafka-console-producer --broker-list localhost:9092 --topic users_clicks --property "parse.key=true" --property "key.separator=:"
> Frank:1
----


Verify that messages are read by different consumers.

=== Read from closest replica

This example shows how to use the feature (since AK 2.4+) for consumers to read messages from the closest replica.

Start a cluster with 3 brokers on 3 different racks, dc1, dc2 and dc3:

[source,bash]
----
scripts/bootstrap-racks.sh
----

Create a topic and assign partition leaderships only on broker 1 and 3 (dc1 and dc3):

[source,bash]
----
docker exec -it broker kafka-reassign-partitions --bootstrap-server broker:9092 --reassignment-json-file /tmp/assignment.json --execute

docker exec -it broker kafka-leader-election --bootstrap-server broker:9092 --topic topic-regional --election-type PREFERRED --partition 0
docker exec -it broker kafka-leader-election --bootstrap-server broker:9092 --topic topic-regional --election-type PREFERRED --partition 1
docker exec -it broker kafka-leader-election --bootstrap-server broker:9092 --topic topic-regional --election-type PREFERRED --partition 2
----

Verify topic describe:

[source,bash]
----
docker exec -it broker kafka-topics --bootstrap-server broker:9092 --topic topic-regional --describe

Topic: topic-regional	TopicId: p-sy0qiQTtSTLTJSG7s7Ew	PartitionCount: 3	ReplicationFactor: 3	Configs:
	Topic: topic-regional	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 2,3,1	Offline:
	Topic: topic-regional	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,1,2	Offline:
	Topic: topic-regional	Partition: 2	Leader: 1	Replicas: 1,3,2	Isr: 1,2,3	Offline:
----

Produce 50 messages:

[source,bash]
----
docker exec -it broker kafka-producer-perf-test --topic topic-regional --num-records 50 --throughput 10 --record-size 1 --producer-props bootstrap.servers=broker:9092
----

Run a consumer that will read messages from broker2 in rack dc2:

[source,bash]
----
cd kafka-consumer
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.consumer.rack.Runner"
----

Teardown:

[source,bash]
----
scripts/tear-down-racks.sh
----

=== Interceptor

This example shows how to create a custom consumer interceptor. class _CreditCardConsumerInterceptor_ will intercept records before deserialization and print headers.

Compile and package:

[source,bash]
----
cd interceptors
mvn clean package
----

Run a kafka producer on listener port 9092:

[source,bash]
----
mvn exec:java -Dexec.mainClass="org.hifly.kafka.interceptor.producer.Runner"
----

Run a kafka consumer on listener port 9092:

[source,bash]
----
mvn exec:java -Dexec.mainClass="org.hifly.kafka.interceptor.consumer.Runner"
----

Verify output:

[source,bash]
----
record headers:RecordHeaders(headers = [], isReadOnly = false)
Group id consumer-interceptor-g2 - Consumer id: consumer-consumer-interceptor-g2-1-0e20b2b6-3269-4bc5-bfdb-ca787cf68aa8 - Topic: test_custom_data - Partition: 0 - Offset: 0 - Key: null - Value: XXXXXX
Consumer 23d06b51-5780-4efc-9c33-a93b3caa3b48 - partition 0 - lastOffset 1
----


=== Python Consumer

Install python lib _confluent-kafka_:

[source,bash]
----
pip install confluent-kafka
----

Create topic:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic kafka-topic --replication-factor 1 --partitions 1
----

Run producer:

[source,bash]
----
cd kafka-python-producer
python producer.py
----

Run consumer:

[source,bash]
----
cd kafka-python-consumer
python consumer.py
----

== Admin & Management

=== Kafka CLI Tools

Kafka CLI are located in _$KAFKA_HOME/bin_ directory.

. _kafka-acls_ - manage acls
. _kafka-topics_ - create, delete, describe, or change a topic
. _kafka-configs_ - create, delete, describe, or change cluster settings
. _kafka-consumer-groups_ - manage consumer groups
. _kafka-console-consumer_ - read data from Kafka topics and outputs it to standard output
. _kafka-console-producer_ - produce data to Kafka topics
. _kafka-consumer-perf-test_ - consume high volumes of data through your Kafka cluster
. _kafka-producer-perf-test_ - produce high volumes of data through your Kafka cluster
. _kafka-avro-console-producer_ - produce Avro data to Kafka topics with a schema _(only with confluent installation)_
. _kafka-avro-console-consumer_ - read Avro data from Kafka topics with a schema and outputs it to standard output _(only with confluent installation)_

=== Kafka Topics: segments and retention

Create a topic _cars_ with retention for old segments set to 5 minutes and size of segments set to 100 KB.

Be aware that _log.retention.check.interval.ms_ is set by default to 5 minutes and this is the frequency in milliseconds that the log cleaner checks whether any log is eligible for deletion.

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars --replication-factor 1 --partitions 1 --config segment.bytes=100000 --config segment.ms=604800000 --config retention.ms=300000 --config retention.bytes=-1
----

Launch a producer perf test:

[source,bash]
----
kafka-producer-perf-test --topic cars --num-records 99999999999999 --throughput -1 --record-size 1 --producer-props bootstrap.servers=localhost:9092
----

Check the log dir for cars topic and wait for deletion of old segments (5 minutes + log cleaner trigger delta)

[source,bash]
----
docker exec -it broker watch ls -ltr /var/lib/kafka/data/cars-0/
----

=== Kafka Admin Client

It uses _org.apache.kafka.clients.admin.AdminClient_ to execute Kafka Admin API.

Operations added:

 - list of cluster nodes
 - list topics

[source,bash]
----
cd admin-client
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.admin.AdminClientWrapper" -Dexec.args="<location_of_admin_property_file>"
----

=== Compression

This example will show that messages sent to the same topic with different _compression.type_ can be read by the same consumer instance.

Compressions supported on producer side are:

 - none (no compression)
 - gzip
 - snappy
 - lz4
 - zstd

Send messages with different compression type and with batching disabled:

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config compression/client-none.properties --property "parse.key=true" --property "key.separator=:"
0:none
----

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config compression/client-gzip.properties --property "parse.key=true" --property "key.separator=:"
1:gzip
----

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config compression/client-snappy.properties --property "parse.key=true" --property "key.separator=:"
2:snappy
----

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config compression/client-lz4.properties --property "parse.key=true" --property "key.separator=:"
3:lz4
----

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config compression/client-zstd.properties --property "parse.key=true" --property "key.separator=:"
4:zstd
----

Run a kafka consumer on _topic1_ topic and see the records:

[source,bash]
----
kafka-console-consumer --topic topic1 --bootstrap-server localhost:9092 --from-beginning
none
gzip
snappy
lz4
zstd
----

== Schema Registry

=== Confluent Avro Specific Record

Implementation of a kafka producer and a kafka consumer using Avro Specific Record for serializing and deserializing.

Confluent schema registry is needed to run the example. +

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Register first version of schema:

[source,bash]
----
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data @src/main/resources/car_v1.avsc \
http://localhost:8081/subjects/cars-value/versions
----


Run the producer:

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerProducer"
----

Run the consumer:

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerConsumer"
----

=== Confluent Avro Compatibility Checks

==== Backward

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Register first version of schema:

[source,bash]
----
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data @avro/car_v1.avsc \
http://localhost:8081/subjects/cars-value/versions
----

Put compatibility on BACKWARD:

[source,bash]
----
curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"compatibility": "BACKWARD"}' \
http://localhost:8081/config/cars-value
----

Get compatibility for subject cars-value:

[source,bash]
----
curl -X GET http://localhost:8081/config/cars-value
----

Run the producer:

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerProducer"
----


Run the consumer (keep it running):

[source,bash]
----
cd confluent-avro-specific-record
mvn clean compile package && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.avro.RunnerConsumer"
----

View the latest schema for this subject:

[source,bash]
----
curl -X GET http://localhost:8081/subjects/cars-value/versions/latest | jq .
----

Register new version of schema, with the addition of a field with default value:

[source,bash]
----
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data @avro/car_v2.avsc \
http://localhost:8081/subjects/cars-value/versions
----

Produce data with new schema id=2 and containing new field:

[source,bash]
----
sh produce-avro-records.sh
----

Verify that consumer will not break and continue to process messages.

Register new version of schema, with the addition of a field with a required value:

[source,bash]
----
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data @avro/car_v3.avsc \
http://localhost:8081/subjects/cars-value/versions
----

you will get an error:

[source,bash]
----
{"error_code":42201,"message":"Invalid schema
----


== Kafka Connect

=== Unixcommand Source Connector

Implementation of a sample Source Connector; it executes _unix commands_ (e.g. _fortune_, _ls -ltr, netstat_) and sends its output to a kafka topic.

IMPORTANT: commands are executed on kafka connect worker node.

This connector relies on Confluent Schema Registry to convert the values using Avro: _CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter_.

Connector config is in _kafka-unixcommand-connector/config/source.quickstart.json_ file.

Parameters for source connector:

- _command_ – unix command to execute (e.g. ls -ltr)
- _topic_ – output topic
- _poll.ms_ – poll interval in milliseconds between every execution

Create the connector package:

[source,bash]
----
cd kafka-unixcommand-connector
mvn clean package
----

Create a connect custom Docker image with the connector installed:

This will create an image based on _confluentinc/cp-kafka-connect-base:XXX_ using a custom _Dockerfile_.
It will use the Confluent utility _confluent-hub install_ to install the plugin in connect.

[source,bash]
----
kafka-unixcommand-connector/build-image.sh
----

Run the Docker container:

[source,bash]
----
scripts/bootstrap-unixcommand-connector.sh
----

Deploy the connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @kafka-unixcommand-connector/config/source.quickstart.json
----

Teardown:

[source,bash]
----
scripts/tear-down-unixcommand-connector.sh
----

=== Custom SMT: composite key from json records.

Implementation of a custom Single Message Transformation (SMT);
it creates a key from a list of json fields taken from message record value. Fields are configurable using SMT property _fields_.

Example:

Original record:

[source,bash]
----
key: null
value: {"FIELD1": "01","FIELD2": "20400","FIELD3": "001","FIELD4": "0006084655017","FIELD5": "20221117","FIELD6": 9000018}
----

Result after SMT:

[source,bash]
----
"transforms.createKey.fields": "FIELD1,FIELD2,FIELD3"

key: 0120400001
value: {"FIELD1": "01","FIELD2": "20400","FIELD3": "001","FIELD4": "0006084655017","FIELD5": "20221117","FIELD6": 9000018}
----

The example applies the SMT to a mongodb sink connector.

Run the example:

[source,bash]
----
scripts/bootstrap-smt-connector.sh
----

A mongodb sink connector will be created with this config:

[source,bash]
----
{
  "name": "mongo-sink",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "topics": "test",
    "connection.uri": "mongodb://admin:password@mongo:27017",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable": false,
    "value.converter.schemas.enable": false,
    "database": "Tutorial2",
    "collection": "pets",
    "transforms": "createKey",
    "transforms.createKey.type": "org.hifly.kafka.smt.KeyFromFields",
    "transforms.createKey.fields": "FIELD1,FIELD2,FIELD3"
  }
}
----

Original json messages will be sent to _test_ topic.
Sink connector will apply the SMT and store the records in mongodb _pets_ collection from _Tutorial2_ database.

Teardown:

[source,bash]
----
scripts/tear-down-smt-connector.sh
----

=== SMT: log records with AOP

Usage of a standard SMT in a mongo sink connector.

_apply_ method for SMT classes in package _org.apache.kafka.connect.transforms_  is intercepted by a Java AOP Aspect
implemented using link:https://www.eclipse.org/aspectj/[AspectJ] framework.

The _@Aspect_, implemented in class _org.hifly.kafka.smt.aspectj.SMTAspect_, logs the input arg (_SinkRecord_ object) to the standard output.


[source,bash]
----
 @Pointcut("execution(* org.apache.kafka.connect.transforms.*.apply(..)) && !execution(* org.apache.kafka.connect.runtime.PredicatedTransformation.apply(..))")
    public void standardMethod() {}

    @Before("standardMethod()")
    public void log(JoinPoint jp) throws Throwable {

        Object[] array = jp.getArgs();
        if(array != null) {
            for(Object tmp: array)
                LOGGER.info(tmp.toString());
        }
    }
----

Connect log will show sink records entries:

[source,bash]
----
SinkRecord{kafkaOffset=0, timestampType=CreateTime} ConnectRecord{topic='test', kafkaPartition=2, key=null, keySchema=Schema{STRING}, value={"FIELD1": "01","FIELD2": "20400","FIELD3": "001","FIELD4": "0006084655017","FIELD5": "20221117","FIELD6": 9000018}, valueSchema=Schema{STRING}, timestamp=1683701851358, headers=ConnectHeaders(headers=)}
----

Run the example:

[source,bash]
----
scripts/bootstrap-smt-aspectj.sh
----

Kafka Connect will start with aspectjweaver java agent:

[source,bash]
----
-Dorg.aspectj.weaver.showWeaveInfo=true -Daj.weaving.verbose=true -javaagent:/usr/share/java/aspectjweaver-1.9.19.jar
----

Aspects are deployed as standard jars and copied to Kafka Connect classpath _/etc/kafka-connect/jars/kafka-smt-aspectj-0.0.1-SNAPSHOT.jar_

A mongodb sink connector will be created with this config:

[source,bash]
----
{
  "name": "mongo-sink",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "topics": "test",
    "connection.uri": "mongodb://admin:password@mongo:27017",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.storage.StringConverter",
    "key.converter.schemas.enable": false,
    "value.converter.schemas.enable": false,
    "database": "Tutorial2",
    "collection": "pets",
    "transforms": "Filter",
    "transforms.Filter.type": "org.apache.kafka.connect.transforms.Filter",
    "transforms.Filter.predicate": "IsFoo",
    "predicates": "IsFoo",
    "predicates.IsFoo.type": "org.apache.kafka.connect.transforms.predicates.TopicNameMatches",
    "predicates.IsFoo.pattern": "test"

  }
}
----

Original json messages will be sent to _test_ topic.
Sink connector will apply the SMT and store the records in mongodb _pets_ collection from _Tutorial2_ database.

Teardown:

[source,bash]
----
scripts/tear-down-smt-aspectj.sh
----

=== Sink Connector Error Handling with a DLQ

MongoDB sink connector example configured to send bad messages to a DLQ topic named _dlq.mongo_

Run the example:

[source,bash]
----
scripts/bootstrap-connect-dlq.sh
----

Create the topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic test --replication-factor 1 --partitions 1
kafka-topics --bootstrap-server localhost:9092 --create --topic dlq.mongo --replication-factor 1 --partitions 1
----

Deploy the connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @kafka-connect-sink-dlq/config/connector_mongo_sink.json
----

A mongodb sink connector will be created with this config:

[source,bash]
----
{
  "name" : "mongo-sample-sink",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "topics": "test",
    "connection.uri": "mongodb://admin:password@mongo:27017",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": false,
    "value.converter.schemas.enable": false,
    "database": "Tutorial2",
    "collection": "pets",
    "errors.tolerance": "all",
    "errors.deadletterqueue.topic.name":"dlq.mongo",
    "errors.deadletterqueue.topic.replication.factor": 1
  }
}
----

Send json messages to _test_ topic (second message is a bad json message):

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic test --property "parse.key=true" --property "key.separator=:"
> 1:{"FIELD1": "01","FIELD2": "20400","FIELD3": "001","FIELD4": "0006084655017","FIELD5": "20221117","FIELD6": 9000018}
> 2:{"FIELD1": "01","FIELD2": "20400","FIELD3": "001","FIELD4": "0006084655017","FIELD5": "20221117",

----

Sink connector will send only the first record in mongodb _pets_ collection from _Tutorial2_ database.

Second message will be stored in _dlq.mongo_ topic.

[source,bash]
----
kafka-console-consumer --topic dlq.mongo --bootstrap-server localhost:9092 --from-beginning
----

Verify that the connector is still in RUNNING status:

[source,bash]
----
curl -v http://localhost:8083/connectors?expand=status
----

Teardown:

[source,bash]
----
scripts/tear-down-connect-dlq.sh
----


=== CDC with Debezium PostgreSQL Source Connector

Usage of Debezium Source connector for PostgreSQL to send RDMS table updates into a kafka topic.

The _debezium/debezium-connector-postgresql:1.7.1_ connector has been installed into connect docker image using confluent hub (see _docker-compose.yml_ file).
More details on the connector are available at: https://docs.confluent.io/debezium-connect-postgres-source/current/overview.html.

Run kafka on port 9092:

[source,bash]
----
scripts/bootstrap-cdc.sh
----

The connector uses _pgoutput_ plugin for replication. This plug-in is always present in PostgreSQL server. The Debezium connector interprets the raw replication event stream directly into change events.

Verify the existence of _account_ table and data in PostgreSQL:

[source,bash]
----
docker exec -it postgres psql -h localhost -p 5432 -U postgres -c 'select * from accounts;'
----

Deploy the connector:

[source,bash]
----
curl -v -X POST -H 'Content-Type: application/json' -d @cdc-debezium-postgres/config/debezium-source-pgsql.json http://localhost:8083/connectors
----

Run a kafka consumer on _postgres.public.accounts_ topic and see the records:

[source,bash]
----
kafka-console-consumer --topic postgres.public.accounts --bootstrap-server localhost:9092 --from-beginning
----

Insert a new record into _account_ table:

[source,bash]
----
docker exec -it postgres psql -h localhost -p 5432 -U postgres -c "insert into accounts (user_id, username, password, email, created_on, last_login) values (3, 'foo3', 'bar3', 'foo3@bar.com', current_timestamp, current_timestamp);"
----

Teardown:

[source,bash]
----
scripts/tear-down-cdc.sh
----

=== Tasks distributions

This example will show how tasks are automatically balanced between Running worker nodes.

A kafka connect cluster will be created with 2 workers, _connect_ and _connect2_ and with a _datagen_ source connector with 4 tasks continuously inserting data.

After some seconds _connect2_ will be stopped and all tasks will be redistributed to _connect_ worker node.

Run sample:

[source,bash]
----
scripts/bootstrap-connect-tasks.sh
----

You will first see tasks distributed between the 2 Running workers:
[source,bash]
----
{"datagen-sample":{"status":{"name":"datagen-sample","connector":{"state":"RUNNING","worker_id":"connect:8083"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"connect:8083"},{"id":1,"state":"RUNNING","worker_id":"connect2:8083"},{"id":2,"state":"RUNNING","worker_id":"connect:8083"},{"id":3,"state":"RUNNING","worker_id":"connect2:8083"}],"type":"source"}}}
----

After stopping _connect2_, you will see tasks only distributed to _connect_ worker:
[source,bash]
----
{"datagen-sample":{"status":{"name":"datagen-sample","connector":{"state":"RUNNING","worker_id":"connect:8083"},"tasks":[{"id":0,"state":"RUNNING","worker_id":"connect:8083"},{"id":1,"state":"RUNNING","worker_id":"connect:8083"},{"id":2,"state":"RUNNING","worker_id":"connect:8083"},{"id":3,"state":"RUNNING","worker_id":"connect:8083"}],"type":"source"}}}
----


Teardown:

[source,bash]
----
scripts/tear-down-connect-tasks.sh
----

== Kafka streams

Implementation of a series of kafka streams topologies.

Execute tests:

[source,bash]
----
cd kafka-streams
mvn clean test
----

=== Events counter Stream +
Count number of events grouped by key.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic counter-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic counter-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.StreamCounter"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic counter-input-topic --property "parse.key=true" --property "key.separator=:"
"John":"transaction_1"
"Mark":"transaction_1"
"John":"transaction_2"
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic counter-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : " --value-deserializer "org.apache.kafka.common.serialization.LongDeserializer"
----

=== Sum values Stream +
Sum values grouping by key.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.StreamSum"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic sum-input-topic --property "parse.key=true" --property "key.separator=:"
"John":1
"Mark":2
"John":5
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic sum-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : " --value-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"
----

=== Cars sensor Stream +
The stream filters out speed data from car data sensor records. Speed limit is set to 150km/h and only events exceeding the limits are filtered out. +
A ktable stores the car info data. +
A left join between the kstream and the ktable produces a new aggregated object published to an output topic.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic carinfo-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic carsensor-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic carsensor-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.CarSensorStream"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic carinfo-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
----

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic carsensor-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","speed":350}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic carsensor-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

=== Cars brand Stream +
The stream splits the original data into 2 different topics, one for Ferrari cars and one for all other car brands.

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic cars-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic ferrari-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic cars-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.stream.CarBrandStream"
----

Send messages to input topic:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic cars-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
2:{"id":"2","brand":"Bugatti","model":"Chiron"}
----

Read from output topics:

[source,bash]
----
kafka-console-consumer --topic ferrari-input-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

[source,bash]
----
kafka-console-consumer --topic cars-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

=== JSONArray Fields removal Processor +

Remove a specific json field from the record and forward it to the next topology node.
This example uses  Kafka streams Processor API.

Execute tests:

[source,bash]
----
cd kafka-streams-processor
mvn clean test
----

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic processor-input-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic processor-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.processor.JSONArrayRemoveProcessorApplication"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic processor-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","brand":"Ferrari","model":"F40"}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic processor-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

=== Expired Messages Processor +

Remove old entries based on time (expiration time 30 seconds) using a punctuator.
This example uses  Kafka streams Processor API.

Execute tests:

[source,bash]
----
cd kafka-streams-processor
mvn clean test
----

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic expired-messages-input-topic--replication-factor <replication_factor> --partitions <number_of_partitions>
kafka-topics --bootstrap-server localhost:9092 --create --topic expired-messages-output-topic --replication-factor <replication_factor> --partitions <number_of_partitions>
----

Run the topology:

[source,bash]
----
cd kafka-streams
mvn clean compile && mvn exec:java -Dexec.mainClass="org.hifly.kafka.demo.streams.processor.ExpiredMessagesApplication"
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic expired-messages-input-topic --property "parse.key=true" --property "key.separator=:"
1:{"id":"1","remote-device":"R01","time":"2021-11-02T02:50:12.208Z"}
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic expired-messages-input-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : "
----

== ksqlDB

=== Saga Pattern Example

Implementation of a sample App (kafka producer and consumer) sending and receiving orders; ksqlDB acts as an orchestrator to coordinate a sample Saga.

Compile:

[source,bash]
----
cd ksqldb-saga-example
mvn schema-registry:download
mvn generate-sources
mvn clean compile
----

Connect to ksqlDB and set auto.offset.reset:

[source,bash]
----
ksql http://ksqldb-server:8088
SET 'auto.offset.reset' = 'earliest';
exit
----

Create DDL on ksqlDB:

[source,bash]
----
cd ksqldb-saga-example/ksql
ksql-statements.sh
----

Create fat jar of Sample application (1 Saga):

[source,bash]
----
cd ksqldb-saga-example
mvn clean compile assembly:single
----

Execute fat jar of Sample application (1 Saga):

[source,bash]
----
cd ksqldb-saga-example
java -jar target/ksqldb-sample-0.0.1-SNAPSHOT-jar-with-dependencies.jar
----

Saga Verification:

Insert entries on ksqlDB:

[source,bash]
----
ksql http://ksqldb-server:8088
----

[source,sql]
----
insert into accounts values('AAA', 'Jimmy Best');
insert into orders values('AAA', 150, 'Item0', 'A123', 'Jimmy Best', 'Transfer funds', '2020-04-22 03:19:51');
insert into orders values('AAA', -110, 'Item1', 'A123', 'amazon.it', 'Purchase', '2020-04-22 03:19:55');
insert into orders values('AAA', -100, 'Item2', 'A123', 'ebike.com', 'Purchase', '2020-04-22 03:19:58');

select * from orders_tx where account_id='AAA' and order_id='A123';
----

[source,java]
----
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": 0, "ACCOUNT": "AAA", "ITEMS": ["Item0"], "ORDER": "A123"}
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": 0, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1"], "ORDER": "A123"}
Order Action:{"TX_ID": "TX_AAA_A123", "TX_ACTION": -1, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1", "Item2"], "ORDER": "A123"}
 --> compensate:{"TX_ID": "TX_AAA_A123", "TX_ACTION": -1, "ACCOUNT": "AAA", "ITEMS": ["Item0", "Item1", "Item2", "ORDER": "A123"}
----

=== Tumbling Window example: heart rate monitoring

Implementation of a tumbling window (1 minute) to monitor heart rate. Values over a threshold of 120 beats per minute are reported.

Connect to ksqlDB and set auto.offset.reset:

[source,bash]
----
ksql http://ksqldb-server:8088
SET 'auto.offset.reset' = 'earliest';
exit
----

Create DDL on ksqlDB:

[source,bash]
----
cd ksqldb-window-tumbling-heartbeat/ksql
ksql-statements.sh
----

Insert entries on ksqlDB:

[source,bash]
----
cd ksqldb-window-tumbling-heartbeat/ksql
ksql-inserts.sh
----

Verify results:

[source,bash]
----
ksql http://ksqldb-server:8088

SELECT person_id,
       beat_over_threshold_count,
       TIMESTAMPTOSTRING(window_start, 'yyy-MM-dd HH:mm:ss', 'UTC') as window_start,
       TIMESTAMPTOSTRING(window_end, 'yyy-MM-dd HH:mm:ss', 'UTC') as window_end
FROM heartbeat_60sec
EMIT CHANGES;

+---------------------------------------------------------+---------------------------------------------------------+---------------------------------------------------------+---------------------------------------------------------+
|PERSON_ID                                                |BEAT_OVER_THRESHOLD_COUNT                                |WINDOW_START                                             |WINDOW_END                                               |
+---------------------------------------------------------+---------------------------------------------------------+---------------------------------------------------------+---------------------------------------------------------+
|MGG1                                                     |3                                                        |2023-02-18 15:10:00                                      |2023-02-18 15:11:00                                      |
|MGG1                                                     |10                                                       |2023-02-18 15:15:00                                      |2023-02-18 15:16:00                                      |
----

=== Session Window example: Vehicle Positions

Implementation of a session window (5 minutes inactive). Vehicle positions (latitude and logitude) are collected and a new window opens when the vehicle does not send its position for 5 minutes. This is considered as a new "trip".

Connect to ksqlDB and set auto.offset.reset:

[source,bash]
----
ksql http://ksqldb-server:8088
SET 'auto.offset.reset' = 'earliest';
exit
----

Create DDL on ksqlDB:

[source,bash]
----
cd ksqldb-window-session-tripsegments/ksql
ksql-statements.sh
----

Insert entries on ksqlDB:

[source,bash]
----
cd ksqldb-window-session-tripsegments/ksql
ksql-inserts.sh
----

Verify results:

[source,bash]
----
ksql http://ksqldb-server:8088

SELECT vehicle_id,
       positions_sent,
       start_latitude,
       start_longitude,
       end_latitude,
       end_longitude,
       TIMESTAMPTOSTRING(window_start, 'yyy-MM-dd HH:mm:ss', 'UTC') as window_start,
       TIMESTAMPTOSTRING(window_end, 'yyy-MM-dd HH:mm:ss', 'UTC') as window_end
FROM trips
EMIT CHANGES;


+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+
|VEHICLE_ID                 |POSITIONS_SENT             |START_LATITUDE             |START_LONGITUDE            |END_LATITUDE               |END_LONGITUDE              |WINDOW_START               |WINDOW_END                 |
+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+
|VH1                        |5                          |42.21                      |17.12                      |42.28                      |17.16                      |2023-02-18 15:10:00        |2023-02-18 15:13:00        |
|VH1                        |2                          |42.31                      |17.17                      |42.33                      |17.18                      |2023-02-18 15:20:00        |2023-02-18 15:22:00        |
----

=== Joins: Devices and temperature measurement

This example shows how to join a STREAM with air temperatures captured by devices and a TABLE containing the information of devices.

Air Temperatures are ingested into a kafka topic _temperature.data_ with a RabbitMQ source connector.

Device Info are ingested into a kafka topic _device_ with a JDBC Source Connector.

Launch Docker Compose:

[source,bash]
----
scripts/bootstrap-ksqldb-join.sh
----

Create input topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic device --replication-factor 1 --partitions 1

kafka-topics --bootstrap-server localhost:9092 --create --topic temperature.data --replication-factor 1 --partitions 1
----

Deploy the JDBC Source connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @ksqldb-join/config/connector_jdbc_source.json
----

Send data to a RabbitMQ queue _temperature.queue_ with a python producer (5 different devices):

[source,bash]
----
pip3 install pika --upgrade
ksqldb-join/config/rabbit_producer.py temperature.queue 5

-->
count:	5
queue:	temperature.queue
Send	{'id': 0, 'body': 35}
Send	{'id': 1, 'body': 18}
Send	{'id': 2, 'body': 2}
Send	{'id': 3, 'body': 5}
Send	{'id': 4, 'body': 32}
Exiting
----

Deploy the RabbitMQ Source connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @ksqldb-join/config/connector_rabbitmq_source.json
----

Execute the ksqlDB statements; Stream _DEVICE_TEMPERATURE_ is a INNER JOIN between DEVICE and TEMPERATURE.DATA

[source,bash]
----
cd ksqldb-join/ksql
./ksql-statements.sh
----

==== Inner Join

Verify the enrichment with a query:

[source,bash]
----
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088 --execute "select * from DEVICE_TEMPERATURE EMIT CHANGES"

-->
+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+
|DEVICE_ID                                                                    |FULLNAME                                                                     |TEMPERATURE                                                                         |
+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+
|1                                                                            |foo11111                                                                     |18                                                                           |
|2                                                                            |foo22222                                                                     |2                                                                            |
----

==== Left Join

Verify the enrichment with a query:

[source,bash]
----
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088 --execute "select * from DEVICE_TEMPERATURE_LJ EMIT CHANGES"

-->
+---------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------+
|DEVICE_ID                                                                  |FULLNAME                                                                   |TEMPERATURE                                                                |
+---------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------+
|0                                                                          |null                                                                       |15                                                                         |
|1                                                                          |foo11111                                                                   |13                                                                         |
|2                                                                          |foo22222                                                                   |16                                                                         |
|3                                                                          |null                                                                       |34                                                                         |
|4                                                                          |null                                                                       |8                                                                          |
----

==== Joins: Devices and devices maintenance - Right Join

This example shows how to join a Table and a Table

Device Info are ingested into a kafka topic _device_ with a JDBC Source Connector.

Maintenances are ingested into a kafka topic _maintenance_ with a JDBC Source Connector.

Launch Docker Compose:

[source,bash]
----
scripts/bootstrap-ksqldb-join.sh
----

Create input topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic device --replication-factor 1 --partitions 1

kafka-topics --bootstrap-server localhost:9092 --create --topic maintenance --replication-factor 1 --partitions 1
----

Deploy the JDBC Source connector:

[source,bash]
----
curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @ksqldb-join/config/connector_jdbc_source.json

curl -X POST -H Accept:application/json -H Content-Type:application/json http://localhost:8083/connectors/ -d @ksqldb-join/config/connector_device_maintenance_jdbc_source.json
----

Execute the ksqlDB statements: TABLE _MAINTENANCE_ RIGHT JOIN TABLE _DEVICE_

[source,bash]
----
cd ksqldb-join/ksql
./ksql-statements-rj.sh
----

[source,bash]
----
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088 --execute "select * from DEVICE_MAINTENANCE EMIT CHANGES"

-->
+---------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------+
|DEVICE_ID                                                                  |FULLNAME                                                                   |MAINTENANCE                                                                |
+---------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------+
|1                                                                          |foo11111                                                                   |2023-03-01 15:00:00 16:00:00                                               |
|2                                                                          |foo22222                                                                   |null                                                                       |
|10                                                                         |foo1010101010                                                              |null                                                                       |
|15                                                                         |foo1515151515                                                              |null                                                                       |

----


== Transactions

=== Transactional producer

see section <<tx_producer>>

=== Read Committed consumer

see section <<readcommitted_consumer>>

=== Kafka Orders App: Example with end-to-end exactly-once semantic between consumer and producer

Example of a cart application implementing end-to-end exactly-once semantic between consumer and producer. +
The ItemsProducer class sends 2 items in a single transaction. +
The ItemsConsumer class receives the items and creates an order containing the items. +
The consumer offset is committed only if the order can be created and sent.

Execute tests:

[source,bash]
----
cd kafka-orders-tx
mvn clean test
----

Execute the ItemsProducer class: 

[source,bash]
----
cd kafka-orders-tx
mvn clean compile && mvn exec:java -Dexec.mainClass="ItemsProducer"
----

Execute the ItemsConsumer class: 

[source,bash]
----
cd kafka-orders-tx
mvn clean compile && mvn exec:java -Dexec.mainClass="ItemsConsumer"
----

== Framework

=== Kafka Spring Boot

Sample of a kafka producer and consumer implemented with Spring Boot 2.x.

Kafka Consumer implements a DLQ for records not processable (after 3 attempts).

Run on your local machine: 

[source,bash]
----
#start a producer on port 8010
cd kafka-springboot-producer
mvn spring-boot:run

#start a consumer on port 8090
cd kafka-springboot-consumer
mvn spring-boot:run

#Send orders (on topic demoTopic)
curl --data '{"id":5, "name": "PS5"}' -H "Content-Type:application/json" http://localhost:8010/api/order

#Send ERROR orders and test DLQ (on topic demoTopic)
curl --data '{"id":5, "name": "ERROR-PS5"}' -H "Content-Type:application/json" http://localhost:8010/api/order
----

=== Kafka Quarkus

Sample of a kafka producer and consumer implemented with Quarkus.
Every 1s a new message is sent to demo topic.

Run on your local machine: 

[source,bash]
----
cd kafka-quarkus
./mvnw clean compile quarkus:dev (debug port 5005)
----

Run on Openshift machine: 

[source,bash]
----
cd kafka-quarkus
./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.kubernetes.deploy=true
----

=== Kafka microprofile2

Sample of a kafka producer and consumer running on an open liberty MicroProfile v2 runtime.

Run on docker: 

[source,bash]
----
#Start a zookeeper container
docker run -d --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 debezium/zookeeper

#Start a kafka container
docker run -d --name my-cluster-kafka-bootstrap -p 9092:9092 --link zookeeper:zookeeper debezium/kafka

#Start a kafka producer container
cd kafka-microprofile2-producer
docker build -t kafka-producer:latest .
docker run -d --name kafka-producer -p 9080:9080 -e KAFKABROKERLIST=my-cluster-kafka-bootstrap:9092 --link my-cluster-kafka-bootstrap:my-cluster-kafka-bootstrap kafka-producer:latest

#Start a kafka consumer container
cd kafka-microprofile2-consumer
docker build -t kafka-consumer:latest .
docker run -d --name kafka-consumer -p 9090:9080 -e KAFKABROKERLIST=my-cluster-kafka-bootstrap:9092 --link my-cluster-kafka-bootstrap:my-cluster-kafka-bootstrap kafka-consumer:latest

#Receive orders
curl -v -X POST http://localhost:9090/kafka-microprofile2-consumer-0.0.1-SNAPSHOT/order

#Send orders (500)
curl -v -X POST http://localhost:9080/kafka-microprofile2-producer-0.0.1-SNAPSHOT/order
----

== Security

=== mTLS listener

This example shows how to define a mTLS listener in kafka on port 9092.

mTLS ensures that both parties in the communication, client and broker, will trust each other exchanging SSL certificates.

Run the example:

[source,bash]
----
scripts/bootstrap-mtls.sh
----

The script will create all the required security files for broker and a client application in _mtls-listener/ssl_ folder.

CA is a fake authority: _C=IT/ST=Lazio/L=Rome/O=Hifly/OU=Hifly_

List of files generated:

 - CA certificate
 - CA key
 - broker csr
 - broker certificate
 - broker keystore
 - broker truststore
 - client csr
 - client certificate
 - client keystore
 - client truststore

Then a broker with mTLS on port 9092 will be started.

To test the mTLS connection, you can try producing some data to a kafka topic with the client keystore already generated and using mTLS _client.properties_

IMPORTANT: mTLS listener has been configured with hostname verification: _ssl.endpoint.identification.algorithm= https_

client CN is: _CN=client,OU=Hifly,O=Hifly,L=Rome,ST=Lazio,C=IT_

[source,bash]
----
cd mtls-listener
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config client.properties --property "parse.key=true" --property "key.separator=:"
1:test
2:test2
----

Teardown:

[source,bash]
----
scripts/tear-down-mtls.sh
----

=== Multiple listeners

This example shows how to define 2 listeners, 1 INTERNAL on port 9092 and 1 for external clients on port 9093 with SASL PLAIN authentication and SSL enabled.

Run the example:

[source,bash]
----
scripts/bootstrap-multi-listener.sh
----

The script will create all the required security files for broker and a client application in _multi-listener/ssl_ folder.

CA is a fake authority: _C=IT/ST=Lazio/L=Rome/O=Hifly/OU=Hifly_

List of files generated:

- CA certificate
- CA key
- broker csr
- broker certificate
- broker keystore
- broker truststore
- client truststore

To test the SASL PLAIN SSL connection, you can try producing some data to a kafka topic using _client.properties_ with user _admin/admin_secret_

[source,bash]
----
cd multi-listener
kafka-console-producer --broker-list broker:9093 --topic topic1 --producer.config client.properties --property "parse.key=true" --property "key.separator=:"
1:test
2:test2
----

Teardown:

[source,bash]
----
scripts/tear-down-multi-listener.sh
----

=== Kafka Custom Authorizer

This example shows how to create a custom authorizer for Kafka

IMPORTANT: this example is only for demo purposes and it's not intended to be deployed in production.

Custom Authorizer _org.hifly.kafka.authorizer.DummyAuthirizer_ extends the basic _AclAuthorizer_ and allows authenticated users to execute
operations on kafka topics without setting any ACLs on them.

Compile and package:

[source,bash]
----
cd authorizers
mvn clean package
cp -rf ./target/authorizers-0.0.1-SNAPSHOT.jar ./jars
----

Run kafka with custom authorizer on port 9092:

[source,bash]
----
scripts/boostrap-auth.sh
----

Run a kafka producer test using the _producer.properties_ on listener port 9092:

_producer.properties_:

[source,bash]
----
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="kafkabroker1" \
  password="kafkabroker1-secret";
----

Producer command:

[source,bash]
----
kafka-console-producer --bootstrap-server localhost:9092 --topic test --producer.config ./src/main/resources/producer.properties
----

Run a kafka consumer test using the _consumer.properties_ on listener port 9092:

_consumer.properties_:

[source,bash]
----
sasl.mechanism=PLAIN
security.protocol=SASL_PLAINTEXT
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="kafkabroker1" \
  password="kafkabroker1-secret";
group.id=test
----

Consumer command:

[source,bash]
----
kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning --consumer.config ./src/main/resources/consumer.properties
----

Teardown:

[source,bash]
----
scripts/tear-down-auth.sh
----

=== Custom Principal Builder

This example shows how to create a custom _KafkaPrincipalBuilder_ to validate the value of _CN_ attribute in SSL certificate.

Only the following CN are allowed:

 - CN=broker
 - CN=client

IMPORTANT: this example is only for demo purposes and it's not intended to be deployed in production.

Run the example:

[source,bash]
----
scripts/bootstrap-principal.sh
----

The script will create all the required security files for broker and 2 client applications in _principal-builder/ssl_ folder.

CA is a fake authority: _C=IT/ST=Lazio/L=Rome/O=Hifly/OU=Hifly_

List of files generated:

- CA certificate
- CA key
- broker csr
- broker certificate
- broker keystore
- broker truststore
- client csr
- client certificate
- client keystore
- client truststore
- client2 csr
- client2 certificate
- client2 keystore
- client2 truststore

Then a broker with mTLS on port 9092 will be started.

IMPORTANT: mTLS listener has been configured with hostname verification: _ssl.endpoint.identification.algorithm= https_

To test the mTLS connection, you can try producing some data to a kafka topic with the client keystore already generated and using mTLS _client.properties_

client CN is: _CN=client,OU=Hifly,O=Hifly,L=Rome,ST=Lazio,C=IT_

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config principal-builder/client.properties --property "parse.key=true" --property "key.separator=:"
1:test
2:test2
----

Messages will be sent with no error; in broker log, you will notice:

[source,bash]
----
[2023-05-31 09:26:03,909] INFO SUBJECT:CN=client,OU=Hifly,O=Hifly,L=Rome,ST=Lazio,C=IT (org.hifly.kafka.principal.CustomPrincipalBuilder)
----

To test the mTLS connection with a not-allowed client, use the property file _client2.properties_

client CN is: _CN=client2,OU=Hifly,O=Hifly,L=Rome,ST=Lazio,C=IT_

[source,bash]
----
kafka-console-producer --broker-list broker:9092 --topic topic1 --producer.config principal-builder/client2.properties --property "parse.key=true" --property "key.separator=:"
1:test
2:test2
----

Messages will not be sent; in broker log, you will notice:

[source,bash]
----
[2023-05-31 09:34:27,868] ERROR Exception while processing request from 192.168.80.3:9092-192.168.80.1:54098-18 (kafka.network.Processor)
java.lang.IllegalStateException: Can't gather SSL certificates!
	at org.hifly.kafka.principal.CustomPrincipalBuilder.build(CustomPrincipalBuilder.java:61)
----

Teardown:

[source,bash]
----
scripts/tear-down-principal.sh
----


=== Kafka OAUTH Authentication KIP-768

This example shows how to configure kafka to use SASL/OAUTHBEARER authentication with Support for OIDC.

To run the sample you need to run Keycloak server and configure openid-connect on it.

Run Keycloak server with PostgreSQL (on port 8080) and Run Kafka with OAUTH listener on port 9093:

[source,bash]
----
scripts/bootstrap-oauth.sh
----

Keycloak setup:

[source,bash]
----

 - Login to http://localhost:8080 (admin/Pa55w0rd)
 - Create a realm called kafka
 - From the Clients tab, create a client with Cliend ID "kafka_user".
 - Change Access Type to Confidential
 - Turn Standard Flow Enabled to OFF
 - Turn Service Accounts Enabled to ON
 - In the Advanced Settings below on the settings tab, set Access Token Lifespan to 10 minutes
 - Switch to the Credentials tab
 - Set Client Authenticator to "Client Id and Secret"
 - Copy the client-secret
 - Save
----

Run a kafka producer test using the _client-oauth-properties_ (add your client_secret into the file) on listener port 9093:

_client-oauth-properties_:

[source,bash]
----
security.protocol=SASL_PLAINTEXT
sasl.mechanism=OAUTHBEARER
sasl.login.callback.handler.class=org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler
sasl.login.connect.timeout.ms=15000
sasl.oauthbearer.token.endpoint.url=http://localhost:8080/auth/realms/kafka/protocol/openid-connect/token
sasl.oauthbearer.expected.audience=account
sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="kafka_user" clientSecret="<client_secret>";
----

Producer command:

[source,bash]
----
kafka-producer-perf-test --topic my_topic --num-records 50 --throughput 10 --record-size 1 --producer-props bootstrap.servers=localhost:9093  --producer.config kafka-oauth-kip-768/client-oauth.properties
----

Teardown:

[source,bash]
----
scripts/tear-down-oauth.sh
----

== Mirroring

=== Mirror Maker 2 tests

Example of a Mirror Maker v2 configuration Active/Active

Run the example:

[source,bash]
----
scripts/bootstrap-mm2.sh
----

A source cluster on port 9092 and a destination cluster on port 9082 will be created.

Create TopicA on source cluster and TopicB on destination cluster:

[source,bash]
----
docker exec broker kafka-topics --bootstrap-server broker:9092 --create --topic TopicA --replication-factor 1 --partitions 3
docker exec broker-destination kafka-topics --bootstrap-server broker-destination:9082 --create --topic TopicB --replication-factor 1 --partitions 3
----

Launch Mirror Maker, both directions:

 - source cluster is identified with name DC-X
 - destination cluster is identified with name DC-Y
 - TopicA will be copied on destination cluster with naming DC-X.TopicA
 - TopicB will be copied on source cluster with naming DC-Y.TopicB

[source,bash]
----
docker exec broker-destination bash -c 'export KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties && connect-mirror-maker /tmp/mm2.properties'
----

Topic list on source cluster:

[source,bash]
----
docker exec broker kafka-topics --bootstrap-server broker:9092 --list

DC-Y.TopicB
DC-Y.checkpoints.internal
TopicA
__consumer_offsets
heartbeats
mm2-configs.DC-Y.internal
mm2-offsets.DC-Y.internal
mm2-status.DC-Y.internal
----

Topic list on destination cluster:

[source,bash]
----
docker exec broker-destination kafka-topics --bootstrap-server broker-destination:9082 --list

DC-X.TopicA
DC-X.checkpoints.internal
TopicB
__consumer_offsets
heartbeats
mm2-configs.DC-X.internal
mm2-offsets.DC-X.internal
mm2-status.DC-X.internal
----

Run Producer Perf Test on source cluster for TopicA:

[source,bash]
----
docker exec broker kafka-producer-perf-test --topic TopicA --num-records 10000 --record-size 100 --throughput -1 --producer-props acks=1 bootstrap.servers=broker:9092 buffer.memory=67108864 batch.size=8196
----

Run Producer Perf Test on destination cluster for TopicB:

[source,bash]
----
docker exec broker-destination kafka-producer-perf-test --topic TopicB --num-records 10000 --record-size 100 --throughput -1 --producer-props acks=1 bootstrap.servers=broker-destination:9082 buffer.memory=67108864 batch.size=8196
----

Verify Topic message size for TopicA (source cluster) and for DCX.TopicA (destination cluster):

[source,bash]
----
docker exec broker ls -ltr /tmp/kraft-combined-logs/TopicA-0
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/DC-X.TopicA-0
docker exec broker ls -ltr /tmp/kraft-combined-logs/TopicA-1
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/DC-X.TopicA-1
docker exec broker ls -ltr /tmp/kraft-combined-logs/TopicA-2
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/DC-X.TopicA-2
----

Verify Topic message size for TopicB (destination cluster) and for DCY.TopicB (source cluster):

[source,bash]
----
docker exec broker ls -ltr /tmp/kraft-combined-logs/DC-Y.TopicB-0
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/TopicB-0
docker exec broker ls -ltr /tmp/kraft-combined-logs/DC-Y.TopicB-1
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/TopicB-1
docker exec broker ls -ltr /tmp/kraft-combined-logs/DC-Y.TopicB-2
docker exec broker-destination ls -ltr /tmp/kraft-combined-logs/TopicB-2
----

Teardown:

[source,bash]
----
scripts/tear-down-mm2.sh
----

== Observability

=== Distributed Tracing with OpenTelemetry for kafka applications

This example shows how to configure OpenTelemetry java auto-instrumentation for a kafka streams application enabling distributed tracing.

In this example it is used _opentelemetry-java-instrumentation_  to inject OpenTelemetry auto instrumentation as a JVM agent requiring no modifications at source code to add the traces.

Apache Kafka producers, consumers and streams are part of the supported libraries as documented at:

https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md#libraries--frameworks

Run OpenTelemetry collector (otlp protocol on port 4317) and Jaeger (on port 16686):

[source,bash]
----
scripts/bootstrap-tracing.sh
----

Create topics:

[source,bash]
----
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-input-topic --replication-factor 1 --partitions 1
kafka-topics --bootstrap-server localhost:9092 --create --topic sum-output-topic --replication-factor 1 --partitions 1
----

Run the kafka stream application with the OpenTelemetry agent:

[source,bash]
----
cd kafka-streams
mvn clean package
cd ..

export OTEL_SERVICE_NAME=stream-sum-service
export OTEL_TRACES_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

java -javaagent:kafka-distributed-tracing/app/opentelemetry-javaagent.jar -Dotel.instrumentation.kafka.enabled=true -Dotel.javaagent.debug=true -jar kafka-streams/target/kafka-streams-0.0.1-SNAPSHOT.jar
----

Send messages to input topics:

[source,bash]
----
kafka-console-producer --broker-list localhost:9092 --topic sum-input-topic --property "parse.key=true" --property "key.separator=:"
"John":1
"Mark":2
"John":5
----

Read from output topic:

[source,bash]
----
kafka-console-consumer --topic sum-output-topic --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property key.separator=" : " --value-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"
----

Open the JaegerUI on http://localhost:16686 and you will have a list of traces from the streaming application.

image::images/traces.png[Traces]

Teardown:

[source,bash]
----
scripts/tear-down-tracing.sh
----

== Performance

=== Producer

Usage of kafka utility _kafka-producer-perf-test_ to test producer performances for java applications.

Kafka cluster is formed with 3 brokers (9092, 9093, 9094).

1 client machine, named _kafka_perf_ is used to run _kafka-producer-perf-test_ against the kafka cluster.

Run the cluster and kafka_perf machine:

[source,bash]
----
scripts/bootstrap-performance.sh
----

==== Scenario 1: effects with compression

Create a topic:

[source,bash]
----
docker exec kafka-perf sh kafka-topics.sh --bootstrap-server broker:9092,broker2:9093,broker3:9094 --create --topic topic-perf --replication-factor 3 --partitions 6 --config min.insync.replicas=2
----

Run a scenario with:

 - 1000000 records
 - record size 2k
 - no compression

[source,bash]
----
docker exec kafka-perf sh kafka-producer-perf-test.sh --topic topic-perf --num-records 1000000 --record-size 2000 --throughput -1 --producer-props bootstrap.servers=broker:9092,broker2:9093,broker3:9094 --print-metrics
----

Run a scenario with:

- 1000000 records
- record size 2k
- lz4 compression

[source,bash]
----
docker exec kafka-perf sh kafka-producer-perf-test.sh --topic topic-perf --num-records 1000000 --record-size 2000 --throughput -1 --producer-props bootstrap.servers=broker:9092,broker2:9093,broker3:9094 compression.type=lz4 --print-metrics
----

Run a scenario with:

- 1000000 records
- record size 2k
- gzip compression

[source,bash]
----
docker exec kafka-perf sh kafka-producer-perf-test.sh --topic topic-perf --num-records 1000000 --record-size 2000 --throughput -1 --producer-props bootstrap.servers=broker:9092,broker2:9093,broker3:9094 compression.type=gzip --print-metrics
----

Run a scenario with:

- 1000000 records
- record size 2k
- snappy compression

[source,bash]
----
docker exec kafka-perf sh kafka-producer-perf-test.sh --topic topic-perf --num-records 1000000 --record-size 2000 --throughput -1 --producer-props bootstrap.servers=broker:9092,broker2:9093,broker3:9094 compression.type=snappy --print-metrics
----

Run a scenario with:

- 1000000 records
- record size 2k
- zstd compression

[source,bash]
----
docker exec kafka-perf sh kafka-producer-perf-test.sh --topic topic-perf --num-records 1000000 --record-size 2000 --throughput -1 --producer-props bootstrap.servers=broker:9092,broker2:9093,broker3:9094 compression.type=zstd --print-metrics
----

Teardown:

[source,bash]
----
scripts/tear-down-performance.sh
----